{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67433933-55d3-4e64-9d0c-198697c12c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mode = 'TEST'\n",
    "%pip install pymysql\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import psycopg2\n",
    "import smtplib as smtp\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import formataddr\n",
    "from email.header import Header\n",
    "from email.mime.application import MIMEApplication\n",
    "from openpyxl import load_workbook\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "# Set working directory - UPDATE THIS PATH\n",
    "os.chdir(r'/Workspace/Ops Analysis/Reports/745 express weekly')\n",
    "\n",
    "# Current datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Define date/time variables\n",
    "TODAY = datetime.datetime.today().strftime('%y%m%d')\n",
    "YES = datetime.date.today() - datetime.timedelta(days=1)\n",
    "LW = datetime.date.today() - datetime.timedelta(days=7)\n",
    "YESTERDAY = YES.strftime('%m/%d/%Y')\n",
    "LWDATE = LW.strftime('%m/%d/%Y')\n",
    "YEST = YES.strftime('%Y-%m-%d')\n",
    "\n",
    "LOG_FILE_NAME = os.getcwd() + r'/logs/' + datetime.datetime.today().strftime('%Y%m%d %H%M%S') + '.log'\n",
    "CSV_FLDR = os.getcwd() + r'/CSV/'\n",
    "SQL_FOLDER = os.getcwd() + r'/SQL/'\n",
    "EXCEL_FLDR = os.getcwd() + r'/XLSX/'\n",
    "\n",
    "# Single Excel template and output path\n",
    "EXCEL_TEMPLATE = os.getcwd() + r'/0745_UAX_WB.xlsx'\n",
    "EXCEL_OUTPUT = TODAY + '0745_UAX_WB_Weekly' + '.xlsx'\n",
    "\n",
    "# SQL file paths for UA/UDH queries\n",
    "UA_SQL_1 = SQL_FOLDER + r'/UA_SQL_1.sql'\n",
    "CNXL_SQL_2 = SQL_FOLDER + r'/CNXL_SQL_2.sql'\n",
    "RES_CNXL_SQL_3 = SQL_FOLDER + r'/RES_CNXL_SQL_3.sql'\n",
    "SCH_DEP_SQL_4 = SQL_FOLDER + r'/SCH_DEP_SQL_4.sql'\n",
    "DIVERTS_SQL_5 = SQL_FOLDER + r'/DIVERTS_SQL_5.sql'\n",
    "RECOVERED_DIVERTS_SQL_6 = SQL_FOLDER + r'/RECOVERED_DIVERTS_SQL_6.sql'\n",
    "UA_HUB_SQL_7 = SQL_FOLDER + r'/UA_HUB_SQL_7.sql'\n",
    "FLIGHTS_NOT_DEPARTED_SQL_8 = SQL_FOLDER + r'/FLIGHTS_NOT_DEPARTED_SQL_8.sql'\n",
    "CANCELLED_FLIGHTS_SQL_9 = SQL_FOLDER + r'/Cancelled_flights_SQL_9.sql'\n",
    "YET_TO_DEPART_SQL_10 = SQL_FOLDER + r'/Yet_to_Depart_flights_SQL_10.sql'\n",
    "RESIDUAL_CANCELS_SQL_11 = SQL_FOLDER + r'/Residual_cancels_SQL_11.sql'\n",
    "REPOSITIONED_FLIGHTS_SQL_12 = SQL_FOLDER + r'/REPOSITIONED_FLIGHTS_SQL_12.sql'\n",
    "REPOSITIONED_COUNT_SQL_13 = SQL_FOLDER + r'/REPOSITIONED_FLIGHTS_COUNT_SQL_13.sql'\n",
    "DIVERTED_FLIGHTS_SQL_14 = SQL_FOLDER + r'/Diverted_flights_SQL_14.sql'\n",
    "UA_SQL_15 = SQL_FOLDER + r'/UA_SQL_15.sql'\n",
    "UA_LINE_SQL = SQL_FOLDER + r'/UA_Line.sql'\n",
    "UA_SEAT_CANCELS_SQL = SQL_FOLDER + r'/UA_Seat_Cancels.sql'\n",
    "CARRIER_DELAYS_SQL = SQL_FOLDER + r'/Carrier_delays.sql'\n",
    "DELAYS_SQL = SQL_FOLDER + r'/Delays_info.sql'\n",
    "\n",
    "# MasFlight SQL files\n",
    "OA_SQL_1 = SQL_FOLDER + r'/OA_SQL_1.sql'\n",
    "OA_SQL_2 = SQL_FOLDER + r'/OA_SQL_2.sql'\n",
    "OA_LINE_SQL = SQL_FOLDER + r'/OA_Line.sql'\n",
    "OA_SEATCNXL_SQL = SQL_FOLDER + r'/OA_SeatCnxl.sql'\n",
    "\n",
    "# Email distribution setup\n",
    "if mode == 'PROD':\n",
    "    listEmailTo = ['harmanpreet.singh@united.com']\n",
    "    listEmailCc = []\n",
    "    listEmailBcc = []\n",
    "else:\n",
    "    listEmailTo = ['harmanpreet.singh@united.com']\n",
    "    listEmailCc = []\n",
    "    listEmailBcc = []\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE_NAME,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logging.info('Starting 745 Express Weekly Report Process')\n",
    "print('Starting 745 Express Weekly Report Process')\n",
    "\n",
    "# Database credentials for UDH\n",
    "udh_username = 'app_noc_daily_ops_call'\n",
    "udh_pass = 'SBhe09r7h20gsav'\n",
    "\n",
    "# Connect to UDH (Redshift)\n",
    "logging.info('Connecting to UDH')\n",
    "print('Connecting to UDH')\n",
    "\n",
    "try:\n",
    "    udh_connection = psycopg2.connect(\n",
    "        user=udh_username,\n",
    "        password=udh_pass,\n",
    "        host='cbs-udh-datalake-prod-redshift-dw1.csjkuisl0oi7.us-east-1.redshift.amazonaws.com',\n",
    "        dbname='udhprod',\n",
    "        port=5439\n",
    "    )\n",
    "    logging.info('UDH connection successful')\n",
    "except Exception as e:\n",
    "    logging.error(f'UDH connection failed: {str(e)}')\n",
    "    raise\n",
    "\n",
    "# Check SuperSnapshot data availability\n",
    "logging.info('Checking SuperSnapshot data availability')\n",
    "print('Checking SuperSnapshot data availability')\n",
    "\n",
    "SS_CNT = pd.read_sql_query(\n",
    "    'select count(*) as cnt from co_prod_vmdb.rtf_flt_leg_operation_ss where report_dt = current_date-1',\n",
    "    udh_connection\n",
    ")\n",
    "SS_CNT_value = SS_CNT['cnt'][0]\n",
    "logging.info(f'Number of rows in SuperSnapshot: {SS_CNT_value}')\n",
    "\n",
    "if SS_CNT_value < 1000:\n",
    "    logging.warning('Latest data unavailable in UDH, checking previous day')\n",
    "    SS_CNT = pd.read_sql_query('select count(*) as cnt from co_prod_vmdb.rtf_flt_leg_operation_ss where report_dt = current_date-2', udh_connection)\n",
    "    SS_CNT_value = SS_CNT['cnt'][0]\n",
    "    \n",
    "    if SS_CNT_value < 1000:\n",
    "        error_msg = 'Latest data unavailable in UDH for both days'\n",
    "        logging.error(error_msg)\n",
    "        raise Exception(error_msg)\n",
    "\n",
    "logging.info('SuperSnapshot data available, proceeding with queries')\n",
    "print('SuperSnapshot data available')\n",
    "\n",
    "def read_sql_file(file_path):\n",
    "    \"\"\"Read SQL file content\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Function to execute query and strip whitespace\n",
    "def clean_and_execute(query, connection, query_name):\n",
    "    \"\"\"Execute query and strip whitespace from string columns\"\"\"\n",
    "    result = pd.read_sql_query(query, connection)\n",
    "    result = result.apply(lambda x: x.astype(str).str.strip() if x.dtype == \"object\" else x)\n",
    "    logging.info(f'{query_name} completed - {len(result)} rows')\n",
    "    print(f'{query_name} done - {len(result)} rows')\n",
    "    return result\n",
    "\n",
    "# Read and execute ALL UDH queries\n",
    "logging.info('Running ALL UDH SQL queries')\n",
    "print('Running ALL UDH SQL queries')\n",
    "\n",
    "ua_sql_1_df = clean_and_execute(read_sql_file(UA_SQL_1), udh_connection, 'UA_SQL_1')\n",
    "cnxl_sql_2_df = clean_and_execute(read_sql_file(CNXL_SQL_2), udh_connection, 'CNXL_SQL_2')\n",
    "res_cnxl_sql_3_df = clean_and_execute(read_sql_file(RES_CNXL_SQL_3), udh_connection, 'RES_CNXL_SQL_3')\n",
    "sch_dep_sql_4_df = clean_and_execute(read_sql_file(SCH_DEP_SQL_4), udh_connection, 'SCH_DEP_SQL_4')\n",
    "diverts_sql_5_df = clean_and_execute(read_sql_file(DIVERTS_SQL_5), udh_connection, 'DIVERTS_SQL_5')\n",
    "recovered_diverts_sql_6_df = clean_and_execute(read_sql_file(RECOVERED_DIVERTS_SQL_6), udh_connection, 'RECOVERED_DIVERTS_SQL_6')\n",
    "ua_hub_sql_7_df = clean_and_execute(read_sql_file(UA_HUB_SQL_7), udh_connection, 'UA_HUB_SQL_7')\n",
    "flights_not_departed_sql_8_df = clean_and_execute(read_sql_file(FLIGHTS_NOT_DEPARTED_SQL_8), udh_connection, 'FLIGHTS_NOT_DEPARTED_SQL_8')\n",
    "cancelled_flights_sql_9_df = clean_and_execute(read_sql_file(CANCELLED_FLIGHTS_SQL_9), udh_connection, 'CANCELLED_FLIGHTS_SQL_9')\n",
    "yet_to_depart_sql_10_df = clean_and_execute(read_sql_file(YET_TO_DEPART_SQL_10), udh_connection, 'YET_TO_DEPART_SQL_10')\n",
    "residual_cancels_sql_11_df = clean_and_execute(read_sql_file(RESIDUAL_CANCELS_SQL_11), udh_connection, 'RESIDUAL_CANCELS_SQL_11')\n",
    "repositioned_flights_sql_12_df = clean_and_execute(read_sql_file(REPOSITIONED_FLIGHTS_SQL_12), udh_connection, 'REPOSITIONED_FLIGHTS_SQL_12')\n",
    "repositioned_count_sql_13_df = clean_and_execute(read_sql_file(REPOSITIONED_COUNT_SQL_13), udh_connection, 'REPOSITIONED_COUNT_SQL_13')\n",
    "diverted_flights_sql_14_df = clean_and_execute(read_sql_file(DIVERTED_FLIGHTS_SQL_14), udh_connection, 'DIVERTED_FLIGHTS_SQL_14')\n",
    "ua_sql_15_df = clean_and_execute(read_sql_file(UA_SQL_15), udh_connection, 'UA_SQL_15')\n",
    "ua_line_df = clean_and_execute(read_sql_file(UA_LINE_SQL), udh_connection, 'UA_LINE')\n",
    "ua_seat_cancels_df = clean_and_execute(read_sql_file(UA_SEAT_CANCELS_SQL), udh_connection, 'UA_SEAT_CANCELS')\n",
    "print(ua_seat_cancels_df)\n",
    "carrier_delays_df = clean_and_execute(read_sql_file(CARRIER_DELAYS_SQL), udh_connection, 'CARRIER_DELAYS')\n",
    "delays_df = clean_and_execute(read_sql_file(DELAYS_SQL), udh_connection, 'DELAYS')\n",
    "\n",
    "# Connect to MasFlight and query data\n",
    "logging.info('Querying MasFlight Data')\n",
    "print('Querying MasFlight Data')\n",
    "\n",
    "mf_connection_string = \"mysql+pymysql://ua_sumit_sharma1:uukexRCsHyAb9kw6rcMX@54.163.228.37:3306/masflightdb_customers_prd?charset=utf8mb4\"\n",
    "mf_engine = create_engine(mf_connection_string)\n",
    "mf_conn = mf_engine.connect()\n",
    "\n",
    "oa_sql_1_df = clean_and_execute(read_sql_file(OA_SQL_1), mf_conn, 'OA_SQL_1')\n",
    "oa_sql_2_df = clean_and_execute(read_sql_file(OA_SQL_2), mf_conn, 'OA_SQL_2')\n",
    "oa_line_df = clean_and_execute(read_sql_file(OA_LINE_SQL), mf_conn, 'OA_LINE')\n",
    "oa_seatcnxl_df = clean_and_execute(read_sql_file(OA_SEATCNXL_SQL), mf_conn, 'OA_SEATCNXL')\n",
    "\n",
    "# Display all rows and columns for oa_sql_1_df\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(oa_sql_1_df)\n",
    "print(oa_seatcnxl_df)\n",
    "# Data validation\n",
    "if len(ua_sql_1_df) <= 10:\n",
    "    error_msg = 'Latest data unavailable in UDH - insufficient rows'\n",
    "    logging.error(error_msg)\n",
    "    print(error_msg)\n",
    "\n",
    "if len(oa_sql_1_df) == 0:\n",
    "    error_msg = 'Latest data unavailable in MasFlight'\n",
    "    logging.error(error_msg)\n",
    "    print(error_msg)\n",
    "\n",
    "# Remove timezone info from datetime columns\n",
    "def remove_timezone_info(df):\n",
    "    \"\"\"Remove timezone info from all datetime columns in a dataframe\"\"\"\n",
    "    for col in df.select_dtypes(include=['datetimetz']).columns:\n",
    "        df[col] = df[col].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "# Function to write DataFrame to existing or new sheet\n",
    "def write_df_to_sheet(ws, df):\n",
    "    \"\"\"Write DataFrame to existing worksheet, clearing existing content\"\"\"\n",
    "    # Clear existing content (keep row 1 for headers)\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
    "        for cell in row:\n",
    "            cell.value = None\n",
    "    \n",
    "    # Write header\n",
    "    for col_idx, col_name in enumerate(df.columns, start=1):\n",
    "        ws.cell(row=1, column=col_idx, value=col_name)\n",
    "    \n",
    "    # Write data rows\n",
    "    for row_idx, row in enumerate(df.itertuples(index=False), start=2):\n",
    "        for col_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=row_idx, column=col_idx, value=value)\n",
    "    \n",
    "    return ws\n",
    "\n",
    "# Function to create new sheet with data\n",
    "def create_sheet_with_data(wb, sheet_name, df):\n",
    "    \"\"\"Create new sheet and write DataFrame content\"\"\"\n",
    "    if sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "        logging.info(f\"{sheet_name} sheet exists - updating\")\n",
    "    else:\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        logging.info(f\"{sheet_name} sheet created\")\n",
    "    \n",
    "    # Write header\n",
    "    for col_idx, col_name in enumerate(df.columns, start=1):\n",
    "        ws.cell(row=1, column=col_idx, value=col_name)\n",
    "    \n",
    "    # Write data rows\n",
    "    for row_idx, row in enumerate(df.itertuples(index=False), start=2):\n",
    "        for col_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=row_idx, column=col_idx, value=value)\n",
    "    \n",
    "    logging.info(f\"{sheet_name} sheet updated with {len(df)} rows\")\n",
    "    print(f\"{sheet_name} sheet updated with {len(df)} rows\")\n",
    "\n",
    "# Update Excel File\n",
    "logging.info('===== STARTING EXCEL UPDATE PROCESS =====')\n",
    "print('===== STARTING EXCEL UPDATE PROCESS =====')\n",
    "\n",
    "# Load the template workbook (keep VBA macros)\n",
    "wb = load_workbook(EXCEL_TEMPLATE, keep_vba=False)\n",
    "\n",
    "# Prepare all dataframes - remove timezone info\n",
    "all_dfs = [\n",
    "    ua_sql_1_df, oa_sql_2_df, cnxl_sql_2_df, res_cnxl_sql_3_df, sch_dep_sql_4_df, oa_sql_1_df,\n",
    "    diverts_sql_5_df, recovered_diverts_sql_6_df, ua_hub_sql_7_df, \n",
    "    flights_not_departed_sql_8_df, repositioned_count_sql_13_df, ua_line_df,\n",
    "    ua_seat_cancels_df, delays_df, carrier_delays_df, cancelled_flights_sql_9_df,\n",
    "    diverted_flights_sql_14_df, yet_to_depart_sql_10_df, residual_cancels_sql_11_df,\n",
    "    repositioned_flights_sql_12_df, oa_line_df, oa_seatcnxl_df, ua_sql_15_df\n",
    "]\n",
    "\n",
    "for df in all_dfs:\n",
    "    remove_timezone_info(df)\n",
    "\n",
    "# ========================================\n",
    "# STEP 1: UPDATE RAW DATA SHEETS FIRST\n",
    "# ========================================\n",
    "logging.info('===== STEP 1: UPDATING RAW DATA SHEETS =====')\n",
    "print('\\n===== STEP 1: UPDATING RAW DATA SHEETS =====')\n",
    "\n",
    "raw_data_sheets = {\n",
    "    \"All_Cancels_Raw\": cancelled_flights_sql_9_df,\n",
    "    \"Yet_to_Depart_Raw\": yet_to_depart_sql_10_df,\n",
    "    \"Residual_Cancels_Raw\": residual_cancels_sql_11_df,\n",
    "    \"Diverted_Flights_Raw\": diverted_flights_sql_14_df,\n",
    "    \"Repositioned_Flights_Raw\": repositioned_flights_sql_12_df\n",
    "}\n",
    "\n",
    "for sheet_name, df in raw_data_sheets.items():\n",
    "    create_sheet_with_data(wb, sheet_name, df)\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: UPDATE TEMPLATE/PROCESSED SHEETS\n",
    "# ========================================\n",
    "logging.info('===== STEP 2: UPDATING TEMPLATE/PROCESSED SHEETS =====')\n",
    "print('\\n===== STEP 2: UPDATING TEMPLATE/PROCESSED SHEETS =====')\n",
    "\n",
    "template_sheets = {\n",
    "    \"UA_SQL_1\": ua_sql_1_df,\n",
    "    \"OA_SQL_1\": oa_sql_1_df,\n",
    "    \"OA_SQL_2\": oa_sql_2_df,\n",
    "    \"CNXL_SQL_2\": cnxl_sql_2_df,\n",
    "    \"RES_CNXL_SQL_3\": res_cnxl_sql_3_df,\n",
    "    \"SCH_DEP_SQL_4\": sch_dep_sql_4_df,\n",
    "    \"TOTAL_DIVERTS_SQL_5\": diverts_sql_5_df,\n",
    "    \"DIVERTS_RECOVERED_SQL_6\": recovered_diverts_sql_6_df,\n",
    "    \"UA_HUB_SQL_7\": ua_hub_sql_7_df,\n",
    "    \"FLIGHTS_NOT_DEPARTED_SQL_8\": flights_not_departed_sql_8_df,\n",
    "    \"REPOSITIONED_FLIGHTS_SQL_13\": repositioned_count_sql_13_df,\n",
    "    \"UA_Line\": ua_line_df,\n",
    "    \"UA_SQL_15\": ua_sql_15_df,\n",
    "    \"UA_Seat_cancels\": ua_seat_cancels_df,\n",
    "    \"Delays_info\": delays_df,\n",
    "    \"Carrier_delays\": carrier_delays_df,\n",
    "    \"OA_Line\": oa_line_df,\n",
    "    \"OA_SeatCnxl\": oa_seatcnxl_df,\n",
    "    \"All_Cancels_Raw\": cancelled_flights_sql_9_df,\n",
    "    \"Yet_to_Depart_Raw\": yet_to_depart_sql_10_df,\n",
    "    \"Residual_Cancels_Raw\": residual_cancels_sql_11_df,\n",
    "    \"Diverted_Flights_Raw\": diverted_flights_sql_14_df,\n",
    "    \"Repositioned_Flights_Raw\": repositioned_flights_sql_12_df\n",
    "}\n",
    "\n",
    "for sheet_name, df in template_sheets.items():\n",
    "    if sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "        write_df_to_sheet(ws, df)\n",
    "        logging.info(f\"Updated existing template sheet: {sheet_name} with {len(df)} rows\")\n",
    "        print(f\"Updated existing template sheet: {sheet_name} with {len(df)} rows\")\n",
    "    else:\n",
    "        create_sheet_with_data(wb, sheet_name, df)\n",
    "\n",
    "# Save the compiled workbook\n",
    "try:\n",
    "    wb.save(EXCEL_OUTPUT)\n",
    "    logging.info(f'✓ Compiled workbook saved: {EXCEL_OUTPUT}')\n",
    "    print(f'\\n✓ Compiled workbook saved: {os.path.basename(EXCEL_OUTPUT)}')\n",
    "    print(f'✓ Total sheets in workbook: {len(wb.sheetnames)}')\n",
    "except Exception as e:\n",
    "    logging.error(f'Error saving workbook: {str(e)}')\n",
    "    raise\n",
    "\n",
    "# Send Email\n",
    "logging.info('Sending email')\n",
    "print('\\nSending email...')\n",
    "\n",
    "server = smtp.SMTP('mailout.ual.com:25')\n",
    "emailFrom = 'harmanpreet.singh@united.com'\n",
    "emailFromHdr = formataddr((str(Header(r'Harman')), emailFrom))\n",
    "\n",
    "# Create message container\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = emailFromHdr\n",
    "msg['To'] = ','.join(listEmailTo)\n",
    "msg['Cc'] = ','.join(listEmailCc)\n",
    "msg['Bcc'] = ','.join(listEmailBcc)\n",
    "\n",
    "if mode == 'PROD':\n",
    "    msg['Subject'] = \"Weekly Report - \" + YEST\n",
    "else:\n",
    "    msg['Subject'] = \"[Databricks Test]: Weekly Report - \" + YEST\n",
    "\n",
    "htmlBdy = \"\"\"<html>\n",
    "    <head></head>\n",
    "    <body>\n",
    "    <p>Hi all,\n",
    "    <br>\n",
    "    <br>Please see below UAX Weekly Snapshot for the week of <strong>\"\"\" + LWDATE + \"\"\" - \"\"\" + YESTERDAY + \"\"\"</strong>.\n",
    "    <br>\n",
    "    <br>The attached workbook contains:\n",
    "    <br><br><strong>Raw Data Sheets (5):</strong>\n",
    "    <br>• All_Cancels_Raw\n",
    "    <br>• Yet_to_Depart_Raw\n",
    "    <br>• Residual_Cancels_Raw\n",
    "    <br>• Diverted_Flights_Raw\n",
    "    <br>• Repositioned_Flights_Raw\n",
    "    <br>\n",
    "    <br><strong>Template/Processed Sheets (16+):</strong>\n",
    "    <br>• UA_SQL_1, OA_SQL_2, CNXL_SQL_2, and other processed data sheets\n",
    "    <br>\n",
    "    <br>Regards,\n",
    "    <br><font size=\"3\"><strong>Ops Planning & Analysis</strong></font>\n",
    "    <br><font size=\"1\">(POWERED BY DATABRICKS)</font>\n",
    "    </p>\n",
    "    </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "body = MIMEText(htmlBdy, 'html')\n",
    "msg.attach(body)\n",
    "\n",
    "# Attach Single Compiled Excel file\n",
    "logging.info('Attaching Compiled Excel')\n",
    "print('Attaching Compiled Excel')\n",
    "xlsx_data = open(EXCEL_OUTPUT, 'rb').read()\n",
    "msgXlsx = MIMEApplication(xlsx_data, _subtype='vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "msgXlsx.add_header('Content-ID', '<xlsx>')\n",
    "msgXlsx.add_header('Content-Disposition', 'attachment', filename=os.path.basename(EXCEL_OUTPUT))\n",
    "msg.attach(msgXlsx)\n",
    "\n",
    "server.sendmail(emailFrom, listEmailTo + listEmailCc + listEmailBcc, msg.as_bytes())\n",
    "logging.info('Email sent and process is completed')\n",
    "print('✓ Email sent')\n",
    "server.quit()\n",
    "\n",
    "# Close connections\n",
    "udh_connection.close()\n",
    "logging.info('===== PROCESS COMPLETED SUCCESSFULLY =====')\n",
    "print('\\n===== PROCESS COMPLETED SUCCESSFULLY =====')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 745 express weekly - testing harman",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
